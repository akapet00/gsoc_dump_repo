{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b19a07b0-53f5-4f44-94b3-f981b63382a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helpers, do not cover in tutorial\n",
    "\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "\n",
    "FIG_PATH = 'figs'\n",
    "SAVE = False\n",
    "\n",
    "def save_fig(fig, fig_name):\n",
    "    \"\"\"Save figure given a figure name in png format.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    fig : matplotlib.figure.Figure\n",
    "        Figure to save.\n",
    "    fig_name : str\n",
    "        File name in string format without the extension.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "    \"\"\"\n",
    "    f = os.path.join(FIG_PATH, fig_name) + '.png'\n",
    "    fig.savefig(f, dpi=144, facecolor='w', edgecolor='w', bbox_inches='tight',\n",
    "                orientation='portrait', format='png', transparent=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edfcf951-99dd-4f2b-a149-a9abf8554363",
   "metadata": {},
   "source": [
    "In this tutorial, we use simulation-based inference on the Hodgkin-Huxley neuron model, where different scenarios are considered.\n",
    "The data are synthetically generated from a model by using the script available [here](https://github.com/brian-team/brian2/blob/master/examples/advanced/modelfitting_sbi.py).\n",
    "This enables the comparisson of the inferred parameter values with the ground truth parameter values as used in the generative model.\n",
    "\n",
    "We start with importing basic packages and modules.\n",
    "Note that ``tutorial_sbi_helpers`` contains three different functions that are used extensively throughout this tutorial.\n",
    "Namely, ``plot_traces`` is used for visualization of the input current and the output voltage traces, and optional time series of spikes or sampled traces obtained by using an estimated posterior.\n",
    "In order to detect and capture spike events in a given voltage trace, we use ``spike_times`` function.\n",
    "Finally, ``plot_cond_coeff_mat`` is used to visualize conditional coerrelation matrix.\n",
    "To check how each of these functions works behind the scenes, download ``tutorial_sbi_helpers.py`` script and go throguh the code yourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f3614b7-b6ac-43a3-a0c6-d2b3ab32e929",
   "metadata": {},
   "outputs": [],
   "source": [
    "from brian2 import *\n",
    "from brian2modelfitting import Inferencer\n",
    "from scipy.stats import kurtosis as kurt\n",
    "from tutorial_sbi_helpers import (spike_times,\n",
    "                                  plot_traces,\n",
    "                                  plot_cond_coeff_mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1cdfd76-b839-4b67-be54-e433e50ce20d",
   "metadata": {},
   "source": [
    "Now, let's load the input current and output voltage traces by using ``NumPy``.\n",
    "Not that all functions available in ``NumPy``, as well as in ``Matplotlib``, are implicitly available after ``brian2`` import."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc3ea42",
   "metadata": {},
   "outputs": [],
   "source": [
    "inp_traces = load('input_traces_synthetic.npy').reshape(1, -1)\n",
    "out_traces = load('output_traces_synthetic.npy').reshape(1, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e159ade9-5403-44ce-aa63-fb33c9a7bfe9",
   "metadata": {},
   "source": [
    "By setting the time step, we are able to set up the time domain.\n",
    "From the time domain we also define the exact time when stimulus started and when it ends by using variables ``stim_start`` and ``stim_end``, respectively.\n",
    "This will come handy later during the feature extraction process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af7c01e9-5d07-471a-a375-27b74a7991a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = 0.1*ms\n",
    "t = arange(0, inp_traces.size*dt/ms, dt/ms)\n",
    "stim_start, stim_end = t[where(inp_traces[0, :] != 0)[0][[0, -1]]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9505599c-e5f0-4dbc-a7cf-4d586536d659",
   "metadata": {},
   "source": [
    "By calling ``plot_traces`` and passing an array of time steps, input current and output voltage traces, we obtain the visualization of a synthetic recording of neural activity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aab5d18-e447-43c5-b03a-e77256414e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plot_traces(t, inp_traces, out_traces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8867b706-8a8c-459d-8584-d9d5ee5b8508",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_fig(fig, 'hh_sbi_recording_traces')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06cee1cd-e63a-4bb8-9b37-312970ba7ba9",
   "metadata": {},
   "source": [
    "# Toy-example: infer two free parameters\n",
    "\n",
    "The first scenario we cover is a simple inference procedure of two unknown parameters in the Hodgkin-Huxley neuron model. The parameters to infer are the maximal value of sodium and potassium electrical conductances.\n",
    "\n",
    "As a standard practice in Brian 2 simulator, we have to define parameters of the model, initial conditions for differential equations that describe the model, and the model itself:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d3f7c91-0e61-4830-bf46-3a6c8d252647",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set parameters of the model\n",
    "E_Na = 53*mV\n",
    "E_K = -107*mV\n",
    "E_l = -70*mV\n",
    "VT = -60.0*mV\n",
    "g_l = 10*nS\n",
    "Cm = 200*pF\n",
    "\n",
    "# set ground truth parameters, which are unknown from the model's perspective\n",
    "ground_truth_params = {'g_Na': 32*uS,\n",
    "                       'g_K': 1*uS}\n",
    "\n",
    "# define initial conditions\n",
    "init_conds = {'v': 'E_l',\n",
    "              'm': '1 / (1 + beta_m / alpha_m)',\n",
    "              'h': '1 / (1 + beta_h / alpha_h)',\n",
    "              'n': '1 / (1 + beta_n / alpha_n)'}\n",
    "\n",
    "# define the Hodgkin-Huxley neuron model\n",
    "eqs = '''\n",
    "    # non-linear set of ordinary differential equations\n",
    "    dv/dt = - (g_Na * m ** 3 * h * (v - E_Na)\n",
    "               + g_K * n ** 4 * (v - E_K)\n",
    "               + g_l * (v - E_l) - I) / Cm : volt\n",
    "    dm/dt = alpha_m * (1 - m) - beta_m * m : 1\n",
    "    dn/dt = alpha_n * (1 - n) - beta_n * n : 1\n",
    "    dh/dt = alpha_h * (1 - h) - beta_h * h : 1\n",
    "    \n",
    "    # time independent rate constants for a channel activation/inactivation\n",
    "    alpha_m = ((-0.32 / mV) * (v - VT - 13.*mV))\n",
    "               / (exp((-(v - VT - 13.*mV)) / (4.*mV)) - 1) / ms : Hz\n",
    "    beta_m = ((0.28/mV) * (v - VT - 40.*mV))\n",
    "              / (exp((v - VT - 40.*mV) / (5.*mV)) - 1) / ms : Hz\n",
    "    alpha_h = 0.128 * exp(-(v - VT - 17.*mV) / (18.*mV)) / ms : Hz\n",
    "    beta_h = 4 / (1 + exp((-(v - VT - 40.*mV)) / (5.*mV))) / ms : Hz\n",
    "    alpha_n = ((-0.032/mV) * (v - VT - 15.*mV))\n",
    "               / (exp((-(v - VT - 15.*mV)) / (5.*mV)) - 1) / ms : Hz\n",
    "    beta_n = 0.5 * exp(-(v - VT - 10.*mV) / (40.*mV)) / ms : Hz\n",
    "        \n",
    "    # free parameters\n",
    "    g_Na : siemens (constant)\n",
    "    g_K : siemens (constant)\n",
    "    '''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4970da98-fe74-46a4-a39c-f390c38806f6",
   "metadata": {},
   "source": [
    "Since the output of the model is extremely high-dimensional, and since we are interested only in a few hand-picked features that will capture the gist of the neuronal activity, we start the inference process by defining a list of summary functions.\n",
    "\n",
    "Each summary feature is obtained by calling a single-valued function on each trace generated by using a sampled prior distribution over unknown parameters.\n",
    "In this case, we consider the maximal value of the action potential, the mean action potential and the standard deviatian of the action potential."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c9df3c1-3133-4ee8-a923-c1f7651cbe2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "v_features = [\n",
    "    # max action potential\n",
    "    lambda x: np.max(x[(t > stim_start) & (t < stim_end)]),\n",
    "    # mean action potential\n",
    "    lambda x: np.mean(x[(t > stim_start) & (t < stim_end)]),\n",
    "    # std of action potential\n",
    "    lambda x: np.std(x[(t > stim_start) & (t < stim_end)]),\n",
    "    # membrane resting potential\n",
    "    lambda x: np.mean(x[(t > 0.1 * stim_start) & (t < 0.9 * stim_start)]),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc0e6055-71dd-45bd-bcf4-5a0067b5147b",
   "metadata": {},
   "source": [
    "## ``Inferencer``\n",
    "\n",
    "The minimum set of arguments for the ``Inferencer`` class constructor are the time step, ``dt``, input data traces, ``input``, output data traces, ``output`` and the model that will be used for the inference process, ``model``.\n",
    "Input and output traces should be shaped so that the number of rows corresponds to each individual observed trace and the number of columns is equal to the number of time steps. \n",
    "\n",
    "Here, we define additional arguments such as ``method`` to define an integration technique used for solving the set of differential equations, ``threshold`` to define a condition that produce a single spike, ``refractory`` to define a condition under which a neuron remains refractory, and ``param_init`` to define a set of initial conditions. We also define the set of summary features that is used to represent the data instead of using the entire trace. Summary features are passed to the inference algorithm via the ``features`` argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d61b3a50-899f-4ae6-9a2b-832758869565",
   "metadata": {},
   "outputs": [],
   "source": [
    "inferencer = Inferencer(dt=dt, model=eqs,\n",
    "                        input={'I': inp_traces*amp},\n",
    "                        output={'v': out_traces*mV},\n",
    "                        features={'v': v_features},\n",
    "                        method='exponential_euler',\n",
    "                        threshold='m > 0.5',\n",
    "                        refractory='m > 0.5',\n",
    "                        param_init=init_conds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a25ab0ec-95fc-45d3-96db-e4d412989170",
   "metadata": {},
   "source": [
    "After the ``Inferencer`` object is created, we begin the inference process by calling ``infer`` method and defining the total number of samples that are used for the learning of a neural density estimator.\n",
    "\n",
    "Neural density estimator learns the probabilistic mapping of the input data, i.e., sampled parameter values given a prior distribution, and the output data, i.e., summary features extracted from the traces, obtained by solving the model with the corresponding set of sampled parameters from the input data.\n",
    "\n",
    "We can choose the inference method and the estimator model , but the only arguments that are required to be passed to ``infer`` method are the number of samples (in case of running the inference process for the first time), ``n_samples``, and upper and lower bounds for each unknown parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9242da33-ae4d-42a1-a9d2-ce18a694f497",
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior = inferencer.infer(n_samples=15_000,\n",
    "                             n_rounds=1,\n",
    "                             inference_method='SNPE',\n",
    "                             density_estimator_model='maf',\n",
    "                             g_Na=[1*uS, 100*uS],\n",
    "                             g_K=[0.1*uS, 10*uS])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "586378e6-866e-4073-b6cf-b4b74e9f1206",
   "metadata": {},
   "source": [
    "After the inference is completed, the estimated posterior distribution over unkown parameters can be analyzed by observing the pairwise relationship between each pair of parameters.\n",
    "But before, we have to draw samples from the estimated posterior as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa80652-e856-48ab-862e-e87d0ddd13da",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = inferencer.sample((10_000, ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b18595-28d1-4739-9fe6-25c47ebd4fd2",
   "metadata": {},
   "source": [
    "The samples are stored inside the ``Inferencer``, and are available through the ``samples`` variable.\n",
    "We create a visual representation of the pairwise relationship of the posterior as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b7736ea-196a-426b-87bc-90c73ad19f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "limits = {'g_Na': [1*uS, 100*uS],\n",
    "          'g_K': [0.1*uS, 10*uS]}\n",
    "labels = {'g_Na': r'$\\overline{g}_\\mathrm{Na}$',\n",
    "          'g_K': r'$\\overline{g}_\\mathrm{K}$'}\n",
    "fig, ax = inferencer.pairplot(limits=limits,\n",
    "                              labels=labels,\n",
    "                              ticks=limits,\n",
    "                              points=ground_truth_params,\n",
    "                              points_offdiag={'markersize': 5},\n",
    "                              points_colors=['C3'],\n",
    "                              figsize=(6, 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b502f5-b71a-4fbf-95d3-93192ab0de40",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_fig(fig, 'hh_sbi_toy_pairplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad7a4a6-185b-4fe6-b113-750223a3d737",
   "metadata": {},
   "source": [
    "Let's zoom in a bit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a451be-d8ac-46c7-b025-ba0c6fd4c5eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = inferencer.pairplot(limits={'g_Na': [30*uS, 34*uS],\n",
    "                                      'g_K': [0.5*uS, 1.5*uS]},\n",
    "                              ticks={'g_Na': [30*uS, 34*uS],\n",
    "                                      'g_K': [0.5*uS, 1.5*uS]},\n",
    "                              labels=labels,\n",
    "                              points=ground_truth_params,\n",
    "                              points_offdiag={'markersize': 5},\n",
    "                              points_colors=['C3'],\n",
    "                              figsize=(6, 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e784f3-b556-4a82-aa20-f140a44854bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_fig(fig, 'hh_sbi_toy_pairplot_zoom_in')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8311a60-7c91-420c-8fe6-eb369568336d",
   "metadata": {},
   "source": [
    "The inferred posterior is plotted against the ground truth parameters, and as can be seen, the ground truth parameters are located in a high-probability region of the estimated distribution.\n",
    "\n",
    "To further substantiate this, let's now see the traces simulated from a single set of parameters sampled from the posterior:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd45762-9b68-4384-9809-877156fbce41",
   "metadata": {},
   "outputs": [],
   "source": [
    "inf_traces = inferencer.generate_traces()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f5a9c97-3d47-4850-ad6b-69c33ee7ec44",
   "metadata": {},
   "source": [
    "We again use the ``plot_traces`` function as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5903084-87b4-4a33-b1e3-56a14a673b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plot_traces(t, inp_traces, out_traces, inf_traces=array(inf_traces/mV))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1517055a-02cf-43cd-9e1c-b9ab748122ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_fig(fig, 'hh_sbi_toy_inf_traces')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb99c8a0-dac6-49cd-a298-0c4cf4401f7c",
   "metadata": {},
   "source": [
    "# Additional free parameters\n",
    "\n",
    "The simple scenarios where only two parameters are considered unkwon works quite well on the synthetic data.\n",
    "What if we have a larger number of unkown parameters? Let us now consider leakage additional unkown parameters for the same model as before. In addition to the unknown maximal values of the electrical conductance of the sodium and potassium channels, the membrane capacity and the maximal value of the electrical conductance of the leakage ion channel are also unknown.\n",
    "\n",
    "We can try to do the same as before but with a bit more training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58889c68-acdf-4f8f-8694-9e43d3cf8d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "del Cm, g_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c4afd78-1b04-4bd0-a818-6c2911eee233",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set parameters, initial condition and the model\n",
    "E_Na = 53*mV\n",
    "E_K = -107*mV\n",
    "E_l = -70*mV\n",
    "VT = -60.0*mV\n",
    "\n",
    "ground_truth_params = {'g_Na': 32*uS,\n",
    "                       'g_K': 1*uS,\n",
    "                       'g_l': 10*nS,\n",
    "                       'Cm': 200*pF}\n",
    "\n",
    "init_conds = {'v': 'E_l',\n",
    "              'm': '1 / (1 + beta_m / alpha_m)',\n",
    "              'h': '1 / (1 + beta_h / alpha_h)',\n",
    "              'n': '1 / (1 + beta_n / alpha_n)'}\n",
    "\n",
    "eqs = '''\n",
    "    # non-linear set of ordinary differential equations\n",
    "    dv/dt = - (g_Na * m ** 3 * h * (v - E_Na)\n",
    "               + g_K * n ** 4 * (v - E_K)\n",
    "               + g_l * (v - E_l) - I) / Cm : volt\n",
    "    dm/dt = alpha_m * (1 - m) - beta_m * m : 1\n",
    "    dn/dt = alpha_n * (1 - n) - beta_n * n : 1\n",
    "    dh/dt = alpha_h * (1 - h) - beta_h * h : 1\n",
    "    \n",
    "    # time independent rate constants for activation and inactivation\n",
    "    alpha_m = ((-0.32 / mV) * (v - VT - 13.*mV))\n",
    "               / (exp((-(v - VT - 13.*mV)) / (4.*mV)) - 1) / ms : Hz\n",
    "    beta_m = ((0.28/mV) * (v - VT - 40.*mV))\n",
    "              / (exp((v - VT - 40.*mV) / (5.*mV)) - 1) / ms : Hz\n",
    "    alpha_h = 0.128 * exp(-(v - VT - 17.*mV) / (18.*mV)) / ms : Hz\n",
    "    beta_h = 4 / (1 + exp((-(v - VT - 40.*mV)) / (5.*mV))) / ms : Hz\n",
    "    alpha_n = ((-0.032/mV) * (v - VT - 15.*mV))\n",
    "               / (exp((-(v - VT - 15.*mV)) / (5.*mV)) - 1) / ms : Hz\n",
    "    beta_n = 0.5 * exp(-(v - VT - 10.*mV) / (40.*mV)) / ms : Hz\n",
    "        \n",
    "    # free parameters\n",
    "    g_Na : siemens (constant)\n",
    "    g_K : siemens (constant)\n",
    "    g_l : siemens (constant)\n",
    "    Cm : farad (constant)\n",
    "    '''\n",
    "\n",
    "# infer the posterior using the same configuration as before\n",
    "inferencer = Inferencer(dt=dt, model=eqs,\n",
    "                        input={'I': inp_traces*amp},\n",
    "                        output={'v': out_traces*mV},\n",
    "                        features={'v': v_features},\n",
    "                        method='exponential_euler',\n",
    "                        threshold='m > 0.5',\n",
    "                        refractory='m > 0.5',\n",
    "                        param_init=init_conds)\n",
    "\n",
    "posterior = inferencer.infer(n_samples=20_000,\n",
    "                             n_rounds=1,\n",
    "                             inference_method='SNPE',\n",
    "                             density_estimator_model='maf',\n",
    "                             g_Na=[1*uS, 100*uS],\n",
    "                             g_K=[0.1*uS, 10*uS],\n",
    "                             g_l=[1*nS, 100*nS],\n",
    "                             Cm=[20*pF, 2*nF])\n",
    "\n",
    "# finally, sample and visualize the posterior distribution\n",
    "samples = inferencer.sample((10_000, ))\n",
    "\n",
    "limits = {'g_Na': [1*uS, 100*uS],\n",
    "          'g_K': [0.1*uS, 10*uS],\n",
    "          'g_l': [1*nS, 100*nS],\n",
    "          'Cm': [20*pF, 2*nF]}\n",
    "labels = {'g_Na': r'$\\overline{g}_\\mathrm{Na}$',\n",
    "          'g_K': r'$\\overline{g}_\\mathrm{K}$',\n",
    "          'g_l': r'$\\overline{g}_{l}$',\n",
    "          'Cm': r'$C_{m}$'}\n",
    "fig, ax = inferencer.pairplot(limits=limits,\n",
    "                              labels=labels,\n",
    "                              ticks=limits,\n",
    "                              points=ground_truth_params,\n",
    "                              points_offdiag={'markersize': 5},\n",
    "                              points_colors=['C3'],\n",
    "                              figsize=(6, 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf32a0a-e693-4226-96d7-e4aa8aaeda00",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_fig(fig, 'hh_sbi_4params_4features_pairplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23fab87a-950e-4259-ac2b-614acebef600",
   "metadata": {},
   "source": [
    "This could have been expected.\n",
    "The posterior distribution is estimated poorly using a simple approach as in the toy example.\n",
    "\n",
    "Yes, we can play around with the hyper-parameters and tuning the neural density estimator, but with this apporach we will not get far.\n",
    "\n",
    "We can, however, try with the non-amortized (or focused) approach, meaning we perform multi-round inference, where each following round will use the posterior from the previous one to sample new input data for the training, rather than using the same prior distribution as defined in the beginning.\n",
    "This approach yields additional advantage - the number of samples may be considerably lower, but it will lead to the posterior that is no longer being amortized, it is accurate only for a specific observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd9187a-f227-4154-860a-1cb78dfd79c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# note that the only difference is the number of rounds of inference\n",
    "posterior = inferencer.infer(n_samples=10_000,\n",
    "                             n_rounds=2,\n",
    "                             inference_method='SNPE',\n",
    "                             density_estimator_model='maf',\n",
    "                             restart=True,\n",
    "                             g_Na=[1*uS, 100*uS],\n",
    "                             g_K=[0.1*uS, 10*uS],\n",
    "                             g_l=[1*nS, 100*nS],\n",
    "                             Cm=[20*pF, 2*nF])\n",
    "\n",
    "samples = inferencer.sample((10_000, ))\n",
    "\n",
    "fig, ax = inferencer.pairplot(limits=limits,\n",
    "                              labels=labels,\n",
    "                              ticks=limits,\n",
    "                              points=ground_truth_params,\n",
    "                              points_offdiag={'markersize': 5},\n",
    "                              points_colors=['C3'],\n",
    "                              figsize=(6, 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f10ff36-b272-44fe-9255-64dd7a7a3a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_fig(fig, 'hh_sbi_4params_multiround_4features_pairplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec7d09d6-b3d9-458a-a572-4589efd4bffc",
   "metadata": {},
   "source": [
    "This seems as a promising approach for parameters that already have the high-probability regions of the posterior distribution around ground-truth values. For other parameters, this leads to further deterioration of posterior estimates.\n",
    "\n",
    "So, we may wonder, how else can we improve the neural density estimator accuracy?\n",
    "\n",
    "Currently, we use only three features to describe extremely complex output of a neural model and should probably create a more comprehensive and more descriptive set of summary features. If we want to include data related to spikes in summary statistics, it is necessary to perform multi-objective optimization since we will observe spike trains as outputs in addition to voltage traces."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb0b6b44-99ff-4bf1-baeb-de2fc74d4e19",
   "metadata": {},
   "source": [
    "## Multi-objective optimization\n",
    "\n",
    "In order to use spikes, we have to have some observation to pass to the ``inferencer``.\n",
    "We can utilize ``spike_times`` function as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9087cb97-2864-4a40-b253-79b1d6272408",
   "metadata": {},
   "outputs": [],
   "source": [
    "spike_times_list = [spike_times(t, out_trace) for out_trace in out_traces]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1cb4dc7-ed00-432d-8bf5-dbf2bb2e8f16",
   "metadata": {},
   "source": [
    "To visually prove that the spikes times are indeed correct, we use ``plot_traces`` again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3927d071-390b-4e20-8d3e-bfd7012ec762",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plot_traces(t, inp_traces, out_traces, spike_times_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "922def1a-6a66-4c1b-900c-8a1dbe2fd9b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_fig(fig, 'hh_sbi_4params_spike_train')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77448aae-7287-4bd5-966e-616b56dc0b12",
   "metadata": {},
   "source": [
    "Now, let us create additional features that will be applied to voltage traces, and a few features that will be applied to spike trains:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a040ac-3dee-42cf-82fd-0e1af5c47f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def voltage_deflection(x):\n",
    "    voltage_base = np.mean(x[t < stim_start])\n",
    "    stim_end_idx = np.where(t >= stim_end)[0][0]\n",
    "    steady_state_voltage_stimend = np.mean(x[stim_end_idx-10:stim_end_idx-5])\n",
    "    return steady_state_voltage_stimend - voltage_base\n",
    "\n",
    "\n",
    "v_features = [\n",
    "    # max action potential\n",
    "    lambda x: np.max(x[(t > stim_start) & (t < stim_end)]),\n",
    "    # mean action potential\n",
    "    lambda x: np.mean(x[(t > stim_start) & (t < stim_end)]),\n",
    "    # std of action potential\n",
    "    lambda x: np.std(x[(t > stim_start) & (t < stim_end)]),\n",
    "    # kurtosis of action potential\n",
    "    lambda x: kurt(x[(t > stim_start) & (t < stim_end)], fisher=False),\n",
    "    # membrane resting potential\n",
    "    lambda x: np.mean(x[(t > 0.1 * stim_start) & (t < 0.9 * stim_start)]),\n",
    "    # the voltage deflection between base and steady-state voltage\n",
    "    voltage_deflection,\n",
    "    ]\n",
    "\n",
    "s_features = [\n",
    "    # number of spikes in a train\n",
    "    lambda x: x.size,\n",
    "    # mean inter-spike interval\n",
    "    lambda x: 0. if np.diff(x).size == 0 else np.mean(diff(x)),\n",
    "    # time to first spike\n",
    "    lambda x: 0. if x.size == 0 else x[0]\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b126c639-df6a-4868-9b72-7c603d80441e",
   "metadata": {},
   "source": [
    "The rest of the inference process stays pretty much the same:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4550ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "inferencer = Inferencer(dt=dt, model=eqs,\n",
    "                        input={'I': inp_traces*amp},\n",
    "                        output={'v': out_traces*mV, 'spikes': spike_times_list},\n",
    "                        features={'v': v_features, 'spikes': s_features},\n",
    "                        method='exponential_euler',\n",
    "                        threshold='m > 0.5',\n",
    "                        refractory='m > 0.5',\n",
    "                        param_init=init_conds)\n",
    "\n",
    "posterior = inferencer.infer(n_samples=20_000,\n",
    "                             n_rounds=1,\n",
    "                             inference_method='SNPE',\n",
    "                             density_estimator_model='maf',\n",
    "                             g_Na=[1*uS, 100*uS],\n",
    "                             g_K=[0.1*uS, 10*uS],\n",
    "                             g_l=[1*nS, 100*nS],\n",
    "                             Cm=[20*pF, 2*nF])\n",
    "\n",
    "samples = inferencer.sample((10_000, ))\n",
    "\n",
    "fig, ax = inferencer.pairplot(limits=limits,\n",
    "                              labels=labels,\n",
    "                              ticks=limits,\n",
    "                              points=ground_truth_params,\n",
    "                              points_offdiag={'markersize': 5},\n",
    "                              points_colors=['C3'],\n",
    "                              figsize=(6, 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f20300-59c3-4bd7-99b1-329071819e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_fig(fig, 'hh_sbi_4params_9features_pairplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a115743-7db0-4a6b-8b73-cd0fc7fe7f47",
   "metadata": {},
   "source": [
    "Let's also visualize the sampled trace, this time using the mean of 10 thousands drawn samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb4601c8-1b99-476f-9733-7a9f3b4be701",
   "metadata": {},
   "outputs": [],
   "source": [
    "inf_traces = inferencer.generate_traces(n_samples=10_000, output_var='v')\n",
    "\n",
    "fig, ax = plot_traces(t, inp_traces, out_traces, inf_traces=array(inf_traces/mV))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a933724-87e7-478d-8d64-593fad7fd557",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_fig(fig, 'hh_sbi_4params_inf_traces')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd500a4-fb9c-43f5-9c51-e0a7a9f2a71b",
   "metadata": {},
   "source": [
    "Okay, now we are clearly getting somewhere and this should be a strong indicatior of the importance of crafting quality summary statistics.\n",
    "\n",
    "Still, the summary statistics can be a huge bottleneck and can set back the training of a neural density estimator. For this reason automatic learning summary features should be considered instead."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f10839-f174-4d88-9d6b-998b7eef50f2",
   "metadata": {},
   "source": [
    "## Automatic feature extraction\n",
    "\n",
    "To enable automatic feature extraction, the ``features`` argument simly should not be defined when instantiating an inferencer object.\n",
    "And that's it. Everything else happens behind the scenes without any need for additional user intervention.\n",
    "If the user wants to gain additional control over the extraction process, in addition to changing the hyperparameters, they can also define their own embedding neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dddb9c3a-8955-4175-8029-d9837df76de9",
   "metadata": {},
   "source": [
    "### Default settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5806d51-3799-413a-94bf-3540afde28ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "inferencer = Inferencer(dt=dt, model=eqs,\n",
    "                        input={'I': inp_traces*amp},\n",
    "                        output={'v': out_traces*mV},\n",
    "                        method='exponential_euler',\n",
    "                        threshold='m > 0.5',\n",
    "                        refractory='m > 0.5',\n",
    "                        param_init=init_conds)\n",
    "\n",
    "posterior = inferencer.infer(n_samples=20_000,\n",
    "                             n_rounds=1,\n",
    "                             inference_method='SNPE',\n",
    "                             density_estimator_model='maf',\n",
    "                             g_Na=[1*uS, 100*uS],\n",
    "                             g_K=[0.1*uS, 10*uS],\n",
    "                             g_l=[1*nS, 100*nS],\n",
    "                             Cm=[20*pF, 2*nF])\n",
    "\n",
    "samples = inferencer.sample((10_000, ))\n",
    "\n",
    "fig, ax = inferencer.pairplot(limits=limits,\n",
    "                              labels=labels,\n",
    "                              ticks=limits,\n",
    "                              points=ground_truth_params,\n",
    "                              points_offdiag={'markersize': 5},\n",
    "                              points_colors=['C3'],\n",
    "                              figsize=(6, 6));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5dbcd6f-862b-456b-b0fd-9335734ab3bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_fig(fig, 'hh_sbi_4params_default_automatic_ext_pairplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c3dc4e-77d4-408e-8dc3-35401708182b",
   "metadata": {},
   "source": [
    "### Custom embedding network\n",
    "\n",
    "Here, we demonstrate how to build a custom summary feature extractor and how to exploit the GPU processing power to speed up the inference process.\n",
    "\n",
    "Note that the use of the GPU will result in the speed-up of computation time only if a custom automatic feature extractor uses techniques that are actually faster to compute on the GPU.\n",
    "\n",
    "For this case, we use the YuleNet, a convolutional neural network, proposed in the paper by Rodrigues and Gramfort 2020, titled *Learning summary features of time series for likelihood free inference*, preprint available at: https://arxiv.org/abs/2012.02807.\n",
    "The authors outline impresive results where the automatic feature extraction by using the YuleNet is capable of outperforming carefully hand-crafted features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1254988-7690-4b63-94a7-106eec22e030",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "class YuleNet(nn.Module):\n",
    "    \"\"\"The summary feature extractor proposed in Rodrigues 2020.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    in_features : int\n",
    "        Number of input features should correspond to the size of a\n",
    "        single output voltage trace.\n",
    "    out_features : int\n",
    "        Number of the features that are used for the inference process.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "    \n",
    "    References\n",
    "    ----------\n",
    "    * Rodrigues, P. L. C. and Gramfort, A. \"Learning summary features\n",
    "      of time series for likelihood free inference\" 3rd Workshop on\n",
    "      Machine Learning and the Physical Sciences (NeurIPS 2020). 2020.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels=1, out_channels=8, kernel_size=64,\n",
    "                               stride=1, padding=32, bias=True)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        pooling1 = 16\n",
    "        self.pool1 = nn.AvgPool1d(kernel_size=pooling1)\n",
    "\n",
    "        self.conv2 = nn.Conv1d(in_channels=8, out_channels=8, kernel_size=64,\n",
    "                               stride=1, padding=32, bias=True)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        pooling2 = int((in_features // pooling1) // 16)\n",
    "        self.pool2 = nn.AvgPool1d(kernel_size=pooling2)\n",
    "\n",
    "        self.dropout = nn.Dropout(p=0.50)\n",
    "        linear_in = 8 * in_features // (pooling1 * pooling2)\n",
    "        self.linear = nn.Linear(in_features=linear_in,\n",
    "                                out_features=out_features)\n",
    "        self.relu3 = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        if x.ndim == 1:\n",
    "            x = x.view(1, 1, -1)\n",
    "        else:\n",
    "            x = x.view(len(x), 1, -1)\n",
    "        x_conv1 = self.conv1(x)\n",
    "        x_relu1 = self.relu1(x_conv1)\n",
    "        x_pool1 = self.pool1(x_relu1)\n",
    "\n",
    "        x_conv2 = self.conv2(x_pool1)\n",
    "        x_relu2 = self.relu2(x_conv2)\n",
    "        x_pool2 = self.pool2(x_relu2)\n",
    "        \n",
    "        x_flatten = x_pool2.view(len(x), 1, -1)\n",
    "        x_dropout = self.dropout(x_flatten)\n",
    "\n",
    "        x = self.relu3(self.linear(x_dropout))\n",
    "        return x.view(len(x), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ffc58b-61eb-4c08-841e-dcfcd77066f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_features = out_traces.shape[1]\n",
    "out_features = 9\n",
    "\n",
    "inferencer = Inferencer(dt=dt, model=eqs,\n",
    "                        input={'I': inp_traces*amp},\n",
    "                        output={'v': out_traces*mV},\n",
    "                        method='exponential_euler',\n",
    "                        threshold='m > 0.5',\n",
    "                        refractory='m > 0.5',\n",
    "                        param_init=init_conds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a88567bd-0227-478b-93bc-c0e79b965899",
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior = inferencer.infer(n_samples=20_000,\n",
    "                             n_rounds=1,\n",
    "                             inference_method='SNPE',\n",
    "                             density_estimator_model='maf',\n",
    "                             inference_kwargs={'embedding_net': YuleNet(in_features, out_features)},\n",
    "                             train_kwargs={'num_atoms': 10,\n",
    "                                           'training_batch_size': 32,\n",
    "                                           'use_combined_loss': True,\n",
    "                                           'discard_prior_samples': True},\n",
    "                             device='gpu',\n",
    "                             g_Na=[1*uS, 100*uS],\n",
    "                             g_K=[0.1*uS, 10*uS],\n",
    "                             g_l=[1*nS, 100*nS],\n",
    "                             Cm=[20*pF, 2*nF])\n",
    "\n",
    "samples = inferencer.sample((10_000, ))\n",
    "\n",
    "fig, ax = inferencer.pairplot(limits=limits,\n",
    "                              labels=labels,\n",
    "                              ticks=limits,\n",
    "                              points=ground_truth_params,\n",
    "                              points_offdiag={'markersize': 5},\n",
    "                              points_colors=['C3'],\n",
    "                              figsize=(6, 6))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f187a7d1-6a0f-4b07-a1cf-b055070fcaae",
   "metadata": {},
   "source": [
    "Except being much faster with additional hyperparameter tuning and more data, high-probability regions of estimated posterior distributions should become more centered around the ground truth values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e994fa-07e6-4325-bfed-950e63294e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_fig(fig, 'hh_sbi_4params_yulenet_pairplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db89b01-f2eb-4bc5-9325-cd20f0a9f077",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
